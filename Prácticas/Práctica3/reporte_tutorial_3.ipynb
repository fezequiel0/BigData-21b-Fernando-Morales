{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Selection For Machine Learning in Python\n",
    "\n",
    "### Univariate selection\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Feature Selection with Univariate Statistical Tests\r\n",
    "from pandas import read_csv\r\n",
    "from numpy import set_printoptions\r\n",
    "from sklearn.feature_selection import SelectKBest\r\n",
    "from sklearn.feature_selection import f_classif\r\n",
    "# load data\r\n",
    "filename = 'pima-indians-diabetes.data.csv'\r\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\r\n",
    "dataframe = read_csv(filename, names=names)\r\n",
    "array = dataframe.values\r\n",
    "X = array[:,0:8]\r\n",
    "Y = array[:,8]\r\n",
    "# feature extraction\r\n",
    "test = SelectKBest(score_func=f_classif, k=4)\r\n",
    "fit = test.fit(X, Y)\r\n",
    "# summarize scores\r\n",
    "set_printoptions(precision=3)\r\n",
    "print(fit.scores_)\r\n",
    "features = fit.transform(X)\r\n",
    "# summarize selected features\r\n",
    "print(features[0:5,:])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141]\n",
      "[[  6.  148.   33.6  50. ]\n",
      " [  1.   85.   26.6  31. ]\n",
      " [  8.  183.   23.3  32. ]\n",
      " [  1.   89.   28.1  21. ]\n",
      " [  0.  137.   43.1  33. ]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recursive feature elimination"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Feature Extraction with RFE\r\n",
    "from pandas import read_csv\r\n",
    "from sklearn.feature_selection import RFE\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "# load data\r\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\r\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\r\n",
    "dataframe = read_csv(url, names=names)\r\n",
    "array = dataframe.values\r\n",
    "X = array[:,0:8]\r\n",
    "Y = array[:,8]\r\n",
    "# feature extraction\r\n",
    "model = LogisticRegression(solver='lbfgs')\r\n",
    "rfe = RFE(model, 3)\r\n",
    "fit = rfe.fit(X, Y)\r\n",
    "print(\"Num Features: %d\" % fit.n_features_)\r\n",
    "print(\"Selected Features: %s\" % fit.support_)\r\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\fezeq\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass n_features_to_select=3 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\fezeq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Principal component analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Feature Extraction with PCA\r\n",
    "import numpy\r\n",
    "from pandas import read_csv\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "# load data\r\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\r\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\r\n",
    "dataframe = read_csv(url, names=names)\r\n",
    "array = dataframe.values\r\n",
    "X = array[:,0:8]\r\n",
    "Y = array[:,8]\r\n",
    "# feature extraction\r\n",
    "pca = PCA(n_components=3)\r\n",
    "fit = pca.fit(X)\r\n",
    "# summarize components\r\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\r\n",
    "print(fit.components_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Feature Importance with Extra Trees Classifier\r\n",
    "from pandas import read_csv\r\n",
    "from sklearn.ensemble import ExtraTreesClassifier\r\n",
    "# load data\r\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\r\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\r\n",
    "dataframe = read_csv(url, names=names)\r\n",
    "array = dataframe.values\r\n",
    "X = array[:,0:8]\r\n",
    "Y = array[:,8]\r\n",
    "# feature extraction\r\n",
    "model = ExtraTreesClassifier(n_estimators=10)\r\n",
    "model.fit(X, Y)\r\n",
    "print(model.feature_importances_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.1   0.226 0.095 0.079 0.078 0.15  0.124 0.148]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comentarios\r\n",
    "\r\n",
    "Este tutorial es mucho más explícito a la hora de explicar el funcionamiento de las técnicas de selección de variables, por lo que el contenido es mucho más comprensible y termina siendo mejor. No tengo muchos comentarios al respecto, más que quedó bastante aclarado el tema, en comparación con el tutorial previo. Al usar el mismo *dataset* a través de todos los ejemplos, se vuelve interesante comparar los resultados de los diferentes métodos de selección de variables."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}